{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108ec259-a6fc-4c31-a2c0-dfbdef8c6b14",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Prediction using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e8466-d7b1-4cf6-b5c0-6d116fb5e4ce",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1da41fa-3bb3-4fe9-a794-7ae7c3e7aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6c8097-1795-47d6-9637-87bd499006d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff565612-e64b-4af6-8104-1a48e039df89",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f7667-8479-48ad-84a2-343dabfd0ce7",
   "metadata": {},
   "source": [
    "**Tokenization** is the process breaking complex data like paragraphs into simple units called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded.\n",
    "1. **Sentence tokenization** : split a paragraph into list of sentences using sent_tokenize() method\n",
    "2. **Word tokenization** : split a sentence into list of words using word_tokenize() method\n",
    "\n",
    "We will be using **Word tokenization** to convert all the text to words\n",
    "\n",
    "Import all the libraries required to perform tokenization on input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21129f87-8502-4e5c-8219-2079ad2e7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abd564-5082-4426-ad04-8ffd8db84e29",
   "metadata": {},
   "source": [
    "**Stop Words** refers to the most common words in a language (such as \"the\", \"a\", \"an\", \"in\") which helps in formation of sentence to make sense, but these words does not provide any significance in language processing so remove it .\n",
    "\n",
    "In computing, stop words are words which are filtered out before or after processing of natural language data (text). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c14c3d-5a9c-4768-9084-1eb123c59c9d",
   "metadata": {},
   "source": [
    "You can check list of stopwords by running below code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbdf922-d8a3-46ab-841e-b75dd863ce98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c165811-6b9b-400b-9b41-9b0683e3f623",
   "metadata": {},
   "source": [
    "**Remove Punctuations**\n",
    "\n",
    "To remove punctuations from the list of words, import all punctuations and add them in the stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc119c9-d394-4dd4-80d6-06f23b60fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stops += punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39fb30-6c3e-4abf-af1a-8dbec7e2f05b",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "**Stemming** is a normalization technique where list of tokenized words are converted into shorten root words to remove redundancy. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.\n",
    "\n",
    "A computer program that stems word may be called a stemmer.\n",
    "\n",
    "A stemmer reduce the words like fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu .\n",
    "\n",
    "It removes suffices, like \"ing\", \"ly\", \"s\", etc. by a simple rule-based approach. It reduces the corpus of words but often the actual words get neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403ef6a-e8ec-4a40-a3d7-3a3d01b71136",
   "metadata": {},
   "source": [
    "**Various Stemming algorithms**\n",
    "1. **Porter stemming algorithm**: This class knows several regular word forms and suffixes with the help of which it can transform the input word to a final stem.\n",
    "2. **Lancaster stemming algorithm**: It was developed at Lancaster University and it is another very common stemming algorithms.\n",
    "NLTK has LancasterStemmer class with the help of which we can easily implement Lancaster Stemmer algorithms for the word we want to stem.\n",
    "3. **Regular Expression stemming algorithm** : With the help of this stemming algorithm, we can construct our own stemmer.\n",
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression.\n",
    "4. **Snowball stemming algorithm**: NLTK has SnowballStemmer class with the help of which we can easily implement Snowball Stemmer algorithms. It supports 15 non-English languages. In order to use this steaming class, we need to create an instance with the name of the language we are using and then call the stem() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f5949b-3d3e-4cca-900e-2944c1cd4a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "player\n",
      "happier\n",
      "happi\n",
      "univers\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "stem_words = [\"play\", \"played\", \"playing\", \"player\", \"happier\", \"happiness\", \"universe\", \"universal\"]\n",
    "from nltk.stem import PorterStemmer #Here we have used the porter stemming algorithm\n",
    "ps = PorterStemmer()\n",
    "for w in stem_words:\n",
    "    print (ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68002623-9bff-410f-b8e3-86d2ca53db60",
   "metadata": {},
   "source": [
    "### **Lemmatization**\n",
    "\n",
    "Major drawback of stemming is it produces Intermediate representation of word. Stemmer may or may not return meaningful word.\n",
    "\n",
    "To overcome this problem , Lemmatization comes into picture.\n",
    "Stemming algorithm works by cutting suffix or prefix from the word.On the contrary Lemmatization consider morphological analysis of the words and returns meaningful word in proper form.\n",
    "\n",
    "The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f0887-9dbf-493c-87ae-cb6d2d25bb30",
   "metadata": {},
   "source": [
    "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714bf74-07c7-4a4a-9bbe-4a04efccf628",
   "metadata": {},
   "source": [
    "### POS Tag\n",
    "\n",
    "Parts of speech Tagging is responsible for reading the text in a language and assigning some specific token (Parts of Speech) to each word.=\n",
    "POS tag tell us about grammatical information of words of the sentence by assigning specific token (Determiner, noun, adjective , adverb ,verb,Personal Pronoun etc.) as tag (DT,NN ,JJ,RB,VB,PRP etc) to each words.\n",
    "\n",
    "Word can have more than one POS depending upon context where it is used. we can use POS tags as statistical NLP tasks it distinguishes sense of word which is very helpful in text realization and infer semantic information from gives text for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d777985-3b8a-43eb-ad6d-883be5e804aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7ba5ec-1e97-43bf-a1cb-f3da1413a2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"good\", pos = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8627bf48-6aa5-4566-9a59-769350862ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"better\", pos = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de3ed4a-0775-4c1d-94c3-ac12a95e8f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'painting'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"painting\", pos = 'n') \n",
    "# Here painting is a noun which means painting can't be converted into paint. For eg\n",
    "# \"This painting is beautiful\". Here painting cannot be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74af887d-b8d6-48fe-ad86-d3ab76f7a0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paint'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"painting\", pos = 'v')\n",
    "# Here painting is a verb which means it can be converted into paint.\n",
    "# \"I love painting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80286816-fec4-47b6-a53f-b14443cd697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    " #creating simple tags to pass into the lemmatizer\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_tweet_data(words):\n",
    "    output_words = []\n",
    "    i = 0\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad8f2e6-f563-4c2c-9e80-8933b77d4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    tweets_data = []\n",
    "    for i in range(df.shape[0]):\n",
    "        tokenised_words = word_tokenize(df['text'][i])\n",
    "        tweets_data.append((clean_tweet_data(tokenised_words), df['airline_sentiment'][i]))\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd87cb1-73dc-4a1d-817b-aa435bd28624",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_clean = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771c6dfd-2044-48da-9e52-48fbf2095890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c797fca-73c0-4478-b0a9-8b208e87ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2)\n",
    "random.shuffle(tweets_data_clean)  \n",
    "## shuffling the training exapmles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7072ebfa-cc2c-48cb-8eaa-865a6c7a4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_train = tweets_data_clean[0:12000]\n",
    "tweets_data_test = tweets_data_clean[12000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c0255-8d7c-4808-876e-736d7e6c1fb8",
   "metadata": {},
   "source": [
    "### Building Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1765e0d5-94e8-4ac6-9efe-a69c4f97ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all words from training data\n",
    "all_words = []\n",
    "for tweet_data in tweets_data_train:\n",
    "    all_words += tweet_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e713de99-bbe1-44ca-992d-8aa8a1097552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13739"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#will retrurn a freq distribution object\n",
    "freq = nltk.FreqDist(all_words)\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2fc4a26-e51c-4f4d-80b1-445129f3626e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #choosing the top 6000 frequency words\n",
    "common = freq.most_common(8000)\n",
    "features = [i[0] for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd946884-3001-439d-bf74-c375a58ace77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will return true/false if the word in present in the tweet text or not\n",
    "def get_feature_dictionary(words):\n",
    "    current_features = {}\n",
    "    words_set = set(words)\n",
    "    for w in features:\n",
    "        current_features[w] = w in words_set\n",
    "    return current_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46ed7a2e-00c4-4144-af87-6b117bb40fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_train = [(get_feature_dictionary(tweet_words), sentiment) for tweet_words, sentiment in tweets_data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b981586-6cab-4517-9f48-c4ec23c7086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_test = [(get_feature_dictionary(tweet_words), sentiment) for tweet_words, sentiment in tweets_data_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d780d00-ee5b-485d-bb42-e98c0f17f8a1",
   "metadata": {},
   "source": [
    "### Classification using NLTK Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d297e1b-77dc-4db6-be74-e92a49a115a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(tweets_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f23df78-7c94-420e-8a58-1136383a1adb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'flight': True,\n",
       "  'united': False,\n",
       "  'usairways': False,\n",
       "  'americanair': True,\n",
       "  'southwestair': False,\n",
       "  'jetblue': False,\n",
       "  \"n't\": False,\n",
       "  'get': False,\n",
       "  \"'s\": False,\n",
       "  'http': False,\n",
       "  'hour': False,\n",
       "  'cancelled': False,\n",
       "  'thanks': False,\n",
       "  'service': False,\n",
       "  'time': False,\n",
       "  'help': False,\n",
       "  'customer': False,\n",
       "  '...': False,\n",
       "  'u': False,\n",
       "  'call': False,\n",
       "  'bag': False,\n",
       "  'wait': False,\n",
       "  'go': False,\n",
       "  'plane': False,\n",
       "  \"'m\": True,\n",
       "  'hold': False,\n",
       "  'need': False,\n",
       "  'amp': False,\n",
       "  'fly': False,\n",
       "  'make': False,\n",
       "  'would': False,\n",
       "  'thank': False,\n",
       "  '2': False,\n",
       "  'still': False,\n",
       "  'one': False,\n",
       "  'day': False,\n",
       "  'please': False,\n",
       "  'delayed': False,\n",
       "  'back': False,\n",
       "  'ca': False,\n",
       "  'gate': False,\n",
       "  'try': True,\n",
       "  'flightled': False,\n",
       "  'virginamerica': False,\n",
       "  'say': False,\n",
       "  'airline': False,\n",
       "  'take': False,\n",
       "  'seat': False,\n",
       "  \"'ve\": False,\n",
       "  'phone': False,\n",
       "  \"''\": False,\n",
       "  '``': False,\n",
       "  'change': False,\n",
       "  'late': False,\n",
       "  'like': False,\n",
       "  'today': False,\n",
       "  'delay': False,\n",
       "  'know': False,\n",
       "  'bad': False,\n",
       "  'guy': False,\n",
       "  'work': False,\n",
       "  'agent': False,\n",
       "  'ticket': False,\n",
       "  'miss': False,\n",
       "  'give': False,\n",
       "  'way': False,\n",
       "  'great': False,\n",
       "  'book': False,\n",
       "  'could': False,\n",
       "  'minute': False,\n",
       "  'airport': False,\n",
       "  'well': False,\n",
       "  '3': False,\n",
       "  'see': False,\n",
       "  'even': False,\n",
       "  'last': False,\n",
       "  'want': False,\n",
       "  'never': False,\n",
       "  'travel': False,\n",
       "  'weather': False,\n",
       "  'check': False,\n",
       "  'tomorrow': False,\n",
       "  'really': False,\n",
       "  'lose': False,\n",
       "  'min': False,\n",
       "  'people': False,\n",
       "  \"'re\": False,\n",
       "  'home': False,\n",
       "  'good': False,\n",
       "  '4': False,\n",
       "  'hr': False,\n",
       "  'email': False,\n",
       "  'issue': False,\n",
       "  'let': False,\n",
       "  'another': False,\n",
       "  'told': False,\n",
       "  'love': False,\n",
       "  'first': False,\n",
       "  'use': False,\n",
       "  'luggage': False,\n",
       "  'new': False,\n",
       "  'number': True,\n",
       "  \"'ll\": False,\n",
       "  'yes': False,\n",
       "  'ever': False,\n",
       "  'dm': False,\n",
       "  'sit': False,\n",
       "  'due': False,\n",
       "  'crew': False,\n",
       "  'someone': False,\n",
       "  'look': False,\n",
       "  '..': False,\n",
       "  'baggage': False,\n",
       "  'next': False,\n",
       "  'two': False,\n",
       "  'reservation': False,\n",
       "  'trip': False,\n",
       "  'come': False,\n",
       "  'tell': False,\n",
       "  'flighted': False,\n",
       "  'much': False,\n",
       "  'passenger': False,\n",
       "  'aa': False,\n",
       "  'right': False,\n",
       "  'experience': False,\n",
       "  'response': False,\n",
       "  'best': False,\n",
       "  'line': False,\n",
       "  '”': False,\n",
       "  'online': False,\n",
       "  'week': False,\n",
       "  'keep': False,\n",
       "  '“': False,\n",
       "  'left': False,\n",
       "  'boarding': False,\n",
       "  'connection': False,\n",
       "  'jfk': False,\n",
       "  'answer': False,\n",
       "  'think': False,\n",
       "  'since': False,\n",
       "  'land': False,\n",
       "  'update': False,\n",
       "  'sure': False,\n",
       "  'long': False,\n",
       "  'follow': False,\n",
       "  'pay': False,\n",
       "  'sent': False,\n",
       "  'refund': False,\n",
       "  'staff': False,\n",
       "  'already': False,\n",
       "  'leave': False,\n",
       "  'stuck': False,\n",
       "  'website': False,\n",
       "  'morning': False,\n",
       "  'find': False,\n",
       "  'system': False,\n",
       "  'flightr': False,\n",
       "  'problem': False,\n",
       "  'care': False,\n",
       "  '1': False,\n",
       "  'tonight': False,\n",
       "  '5': False,\n",
       "  'board': False,\n",
       "  'yet': False,\n",
       "  'nothing': False,\n",
       "  'show': False,\n",
       "  'put': False,\n",
       "  'wo': False,\n",
       "  'night': False,\n",
       "  'ask': False,\n",
       "  'hotel': False,\n",
       "  \"'d\": False,\n",
       "  '’': False,\n",
       "  'us': False,\n",
       "  'dfw': False,\n",
       "  'fleet': False,\n",
       "  'fleek': False,\n",
       "  'hope': False,\n",
       "  'mile': False,\n",
       "  'attendant': False,\n",
       "  'rebooked': False,\n",
       "  'booking': False,\n",
       "  'problems': False,\n",
       "  'appreciate': False,\n",
       "  'ago': False,\n",
       "  'year': False,\n",
       "  'rude': False,\n",
       "  'air': False,\n",
       "  'flt': False,\n",
       "  'free': False,\n",
       "  'charge': False,\n",
       "  'talk': False,\n",
       "  'class': False,\n",
       "  'fix': False,\n",
       "  'nice': False,\n",
       "  'send': False,\n",
       "  'info': False,\n",
       "  'pilot': False,\n",
       "  'wifi': False,\n",
       "  'rebook': False,\n",
       "  'point': False,\n",
       "  'arrive': False,\n",
       "  'awesome': False,\n",
       "  'anything': False,\n",
       "  'also': False,\n",
       "  'connect': False,\n",
       "  'voucher': False,\n",
       "  'start': False,\n",
       "  'upgrade': False,\n",
       "  'credit': False,\n",
       "  'tweet': False,\n",
       "  'fail': False,\n",
       "  'employee': False,\n",
       "  'lax': False,\n",
       "  'thing': False,\n",
       "  'business': False,\n",
       "  'option': False,\n",
       "  'frustrate': False,\n",
       "  'able': False,\n",
       "  'name': False,\n",
       "  'app': False,\n",
       "  'add': True,\n",
       "  'happen': False,\n",
       "  'finally': False,\n",
       "  '30': False,\n",
       "  'anyone': False,\n",
       "  'receive': False,\n",
       "  'phl': False,\n",
       "  '....': False,\n",
       "  'offer': False,\n",
       "  'do': False,\n",
       "  'rep': False,\n",
       "  'via': False,\n",
       "  'fee': False,\n",
       "  'every': False,\n",
       "  '10': False,\n",
       "  'status': False,\n",
       "  'many': False,\n",
       "  'understand': False,\n",
       "  'contact': False,\n",
       "  '6': False,\n",
       "  'speak': False,\n",
       "  'delta': False,\n",
       "  'claim': False,\n",
       "  'person': False,\n",
       "  'stop': False,\n",
       "  'open': False,\n",
       "  'team': False,\n",
       "  'terrible': False,\n",
       "  'checked': False,\n",
       "  '1st': False,\n",
       "  'available': False,\n",
       "  'yesterday': False,\n",
       "  'suck': False,\n",
       "  'chance': False,\n",
       "  'departure': False,\n",
       "  'thx': False,\n",
       "  'sfo': False,\n",
       "  'gt': False,\n",
       "  'always': False,\n",
       "  'return': False,\n",
       "  'sorry': False,\n",
       "  'kid': False,\n",
       "  'without': False,\n",
       "  'ok': False,\n",
       "  'hi': False,\n",
       "  'early': False,\n",
       "  'instead': False,\n",
       "  'disappointed': False,\n",
       "  'helpful': False,\n",
       "  'different': False,\n",
       "  'money': False,\n",
       "  'clt': False,\n",
       "  'suppose': False,\n",
       "  'southwest': False,\n",
       "  'extra': False,\n",
       "  'earlier': False,\n",
       "  'site': False,\n",
       "  'though': False,\n",
       "  'rt': False,\n",
       "  'ord': False,\n",
       "  'paid': False,\n",
       "  'dca': False,\n",
       "  'card': False,\n",
       "  'happy': False,\n",
       "  'deal': False,\n",
       "  'something': False,\n",
       "  'friend': False,\n",
       "  'direct': False,\n",
       "  'almost': False,\n",
       "  'company': False,\n",
       "  'everyone': False,\n",
       "  'poor': False,\n",
       "  'boston': False,\n",
       "  'mean': False,\n",
       "  'unacceptable': False,\n",
       "  'ridiculous': False,\n",
       "  'actually': False,\n",
       "  'respond': False,\n",
       "  'seem': False,\n",
       "  'ground': False,\n",
       "  'question': False,\n",
       "  'member': False,\n",
       "  'oh': False,\n",
       "  'strand': False,\n",
       "  'hung': False,\n",
       "  'chicago': False,\n",
       "  'w/': False,\n",
       "  'job': False,\n",
       "  'pas': False,\n",
       "  'full': False,\n",
       "  'wife': False,\n",
       "  '20': False,\n",
       "  'least': False,\n",
       "  'reply': False,\n",
       "  'dallas': False,\n",
       "  'schedule': False,\n",
       "  'message': False,\n",
       "  'month': False,\n",
       "  'horrible': False,\n",
       "  'policy': False,\n",
       "  'old': False,\n",
       "  'found': False,\n",
       "  'american': False,\n",
       "  'plan': False,\n",
       "  'twitter': False,\n",
       "  'destinationdragons': False,\n",
       "  'snow': False,\n",
       "  'run': False,\n",
       "  'reason': False,\n",
       "  'family': False,\n",
       "  'move': False,\n",
       "  'expect': False,\n",
       "  'allow': False,\n",
       "  'cause': False,\n",
       "  'account': False,\n",
       "  'seriously': False,\n",
       "  'request': False,\n",
       "  'hang': False,\n",
       "  'tarmac': False,\n",
       "  '8': False,\n",
       "  'ur': False,\n",
       "  'treat': False,\n",
       "  'ewr': False,\n",
       "  'vacation': False,\n",
       "  'san': False,\n",
       "  'amaze': False,\n",
       "  'denver': False,\n",
       "  'stay': False,\n",
       "  'vegas': False,\n",
       "  'confirmation': False,\n",
       "  'bos': False,\n",
       "  'away': False,\n",
       "  'feel': False,\n",
       "  'big': False,\n",
       "  'far': False,\n",
       "  'reschedule': False,\n",
       "  'wrong': False,\n",
       "  'little': False,\n",
       "  'confirm': False,\n",
       "  'food': False,\n",
       "  '7': False,\n",
       "  'enough': False,\n",
       "  'cost': False,\n",
       "  'provide': False,\n",
       "  'car': False,\n",
       "  'might': False,\n",
       "  'three': False,\n",
       "  'desk': False,\n",
       "  'past': False,\n",
       "  'end': False,\n",
       "  'maybe': False,\n",
       "  'complaint': False,\n",
       "  'half': False,\n",
       "  'lot': False,\n",
       "  'cool': False,\n",
       "  'handle': False,\n",
       "  'link': False,\n",
       "  'broken': False,\n",
       "  'switch': False,\n",
       "  '45': False,\n",
       "  'twice': False,\n",
       "  '--': False,\n",
       "  'apology': False,\n",
       "  'nyc': False,\n",
       "  'error': False,\n",
       "  'row': False,\n",
       "  'bc': False,\n",
       "  'runway': False,\n",
       "  'together': False,\n",
       "  'mechanical': False,\n",
       "  'charlotte': False,\n",
       "  'hey': False,\n",
       "  'less': False,\n",
       "  'place': False,\n",
       "  'destination': False,\n",
       "  'possible': False,\n",
       "  'ua': False,\n",
       "  'life': False,\n",
       "  'terminal': False,\n",
       "  'pls': False,\n",
       "  'soon': False,\n",
       "  'guess': False,\n",
       "  'worst': False,\n",
       "  'bring': False,\n",
       "  'may': False,\n",
       "  'drop': False,\n",
       "  'longer': False,\n",
       "  'city': False,\n",
       "  'heard': False,\n",
       "  'real': False,\n",
       "  'computer': False,\n",
       "  'address': False,\n",
       "  'airways': False,\n",
       "  'reach': True,\n",
       "  'around': False,\n",
       "  'believe': False,\n",
       "  'idea': False,\n",
       "  'award': False,\n",
       "  'newark': False,\n",
       "  'second': False,\n",
       "  '50': False,\n",
       "  '24': False,\n",
       "  'room': False,\n",
       "  'fare': False,\n",
       "  'pick': False,\n",
       "  '200': False,\n",
       "  'drive': False,\n",
       "  'phx': False,\n",
       "  'sleep': False,\n",
       "  'everything': False,\n",
       "  'w': False,\n",
       "  'watch': False,\n",
       "  'iad': False,\n",
       "  'miami': False,\n",
       "  'monday': False,\n",
       "  'plus': False,\n",
       "  'lol': False,\n",
       "  '40': False,\n",
       "  'close': False,\n",
       "  'sat': False,\n",
       "  'hard': False,\n",
       "  'price': False,\n",
       "  'screw': False,\n",
       "  'load': False,\n",
       "  'depart': False,\n",
       "  'houston': False,\n",
       "  'else': False,\n",
       "  'hear': False,\n",
       "  'lack': False,\n",
       "  'awful': False,\n",
       "  'deliver': False,\n",
       "  'disconnect': False,\n",
       "  'figure': False,\n",
       "  'assistance': False,\n",
       "  'dc': False,\n",
       "  'turn': False,\n",
       "  'process': False,\n",
       "  'cust': False,\n",
       "  'im': False,\n",
       "  'thought': False,\n",
       "  'glad': False,\n",
       "  'lga': False,\n",
       "  'lie': False,\n",
       "  'share': False,\n",
       "  'international': False,\n",
       "  'reflight': False,\n",
       "  'easy': False,\n",
       "  'forward': False,\n",
       "  'jet': False,\n",
       "  'quick': False,\n",
       "  'loyal': False,\n",
       "  'leg': False,\n",
       "  'ceo': False,\n",
       "  '1/2': False,\n",
       "  'world': False,\n",
       "  'entire': False,\n",
       "  'group': False,\n",
       "  'date': False,\n",
       "  '25': False,\n",
       "  'wish': False,\n",
       "  '2nd': False,\n",
       "  'thru': False,\n",
       "  'swa': False,\n",
       "  '15': False,\n",
       "  'maintenance': False,\n",
       "  'information': False,\n",
       "  '100': False,\n",
       "  'high': False,\n",
       "  'fll': False,\n",
       "  'form': False,\n",
       "  'child': False,\n",
       "  'word': False,\n",
       "  'head': False,\n",
       "  'la': False,\n",
       "  'route': False,\n",
       "  'bought': False,\n",
       "  'waste': False,\n",
       "  'unitedairlines': False,\n",
       "  'counting': False,\n",
       "  'buy': False,\n",
       "  'busy': False,\n",
       "  'human': False,\n",
       "  'den': False,\n",
       "  'na': False,\n",
       "  'philly': False,\n",
       "  'club': False,\n",
       "  'force': False,\n",
       "  'sunday': False,\n",
       "  'standby': False,\n",
       "  'carry': False,\n",
       "  'case': False,\n",
       "  'communication': False,\n",
       "  'iah': False,\n",
       "  'wow': False,\n",
       "  'bwi': False,\n",
       "  'correct': False,\n",
       "  'fine': False,\n",
       "  'resolve': False,\n",
       "  'airlines': False,\n",
       "  'empty': False,\n",
       "  'pretty': False,\n",
       "  '9': False,\n",
       "  'blue': False,\n",
       "  'trying': False,\n",
       "  'imaginedragons': False,\n",
       "  '800': False,\n",
       "  'flyer': False,\n",
       "  'beyond': False,\n",
       "  'purchase': False,\n",
       "  'rather': False,\n",
       "  'flighting': False,\n",
       "  'ready': False,\n",
       "  'flew': False,\n",
       "  'hop': False,\n",
       "  'happens': False,\n",
       "  'joke': False,\n",
       "  'hopefully': False,\n",
       "  'counter': False,\n",
       "  'file': False,\n",
       "  'spent': False,\n",
       "  'list': False,\n",
       "  'sw': False,\n",
       "  \"y'all\": False,\n",
       "  'asap': False,\n",
       "  'either': False,\n",
       "  'record': False,\n",
       "  'detail': False,\n",
       "  'support': False,\n",
       "  'okay': False,\n",
       "  'wtf': False,\n",
       "  'tsa': False,\n",
       "  'got': False,\n",
       "  'flightlations': False,\n",
       "  'usair': False,\n",
       "  'lt': False,\n",
       "  'future': False,\n",
       "  'worry': False,\n",
       "  'refuse': False,\n",
       "  'companion': False,\n",
       "  'storm': False,\n",
       "  'yeah': False,\n",
       "  'fuck': False,\n",
       "  'currently': False,\n",
       "  '12': False,\n",
       "  'set': False,\n",
       "  'flown': False,\n",
       "  'automate': False,\n",
       "  'report': False,\n",
       "  'fault': False,\n",
       "  'drink': False,\n",
       "  'live': False,\n",
       "  'traveler': False,\n",
       "  'rock': False,\n",
       "  'sad': False,\n",
       "  'hate': False,\n",
       "  'layover': False,\n",
       "  'cant': False,\n",
       "  'checkin': False,\n",
       "  'atl': False,\n",
       "  'access': False,\n",
       "  'upset': False,\n",
       "  'must': False,\n",
       "  'super': False,\n",
       "  'completely': False,\n",
       "  'safety': False,\n",
       "  'submit': False,\n",
       "  'explain': False,\n",
       "  '1.5': False,\n",
       "  'supervisor': False,\n",
       "  'ruin': False,\n",
       "  'nashville': False,\n",
       "  'wonder': False,\n",
       "  'fill': False,\n",
       "  'situation': False,\n",
       "  '2015': False,\n",
       "  'probably': False,\n",
       "  'whole': False,\n",
       "  'mess': False,\n",
       "  'customerservice': False,\n",
       "  'sign': False,\n",
       "  'part': False,\n",
       "  'las': False,\n",
       "  'fun': False,\n",
       "  'clothes': False,\n",
       "  'pm': False,\n",
       "  'read': False,\n",
       "  'step': False,\n",
       "  'letter': False,\n",
       "  'small': False,\n",
       "  'note': False,\n",
       "  'luck': False,\n",
       "  'tv': False,\n",
       "  'tuesday': False,\n",
       "  'space': False,\n",
       "  'door': False,\n",
       "  'fact': False,\n",
       "  'save': False,\n",
       "  'zero': False,\n",
       "  'cabin': False,\n",
       "  'apparently': False,\n",
       "  'sense': False,\n",
       "  'mileage': False,\n",
       "  'arrival': False,\n",
       "  'luv': False,\n",
       "  'b': False,\n",
       "  'orlando': False,\n",
       "  'austin': False,\n",
       "  'compensation': False,\n",
       "  'continue': False,\n",
       "  'winter': False,\n",
       "  'yall': False,\n",
       "  'gon': False,\n",
       "  'pass': False,\n",
       "  'aircraft': False,\n",
       "  'notification': False,\n",
       "  'weekend': False,\n",
       "  'huge': False,\n",
       "  'several': False,\n",
       "  'complete': False,\n",
       "  'matter': False,\n",
       "  '11': False,\n",
       "  'choice': False,\n",
       "  'post': False,\n",
       "  'mom': False,\n",
       "  'inconvenience': False,\n",
       "  'original': False,\n",
       "  'notify': False,\n",
       "  'id': False,\n",
       "  'atlanta': False,\n",
       "  'state': False,\n",
       "  'promise': False,\n",
       "  'platinum': False,\n",
       "  'man': False,\n",
       "  'kind': False,\n",
       "  'rdu': False,\n",
       "  'knew': False,\n",
       "  'sell': False,\n",
       "  'news': False,\n",
       "  'priority': False,\n",
       "  'lounge': False,\n",
       "  'medium': False,\n",
       "  'write': False,\n",
       "  'meeting': False,\n",
       "  'front': False,\n",
       "  'multiple': False,\n",
       "  'r': False,\n",
       "  'include': False,\n",
       "  'unfortunately': False,\n",
       "  'stand': False,\n",
       "  'wall': False,\n",
       "  'cross': False,\n",
       "  'notice': False,\n",
       "  'daughter': False,\n",
       "  'looking': False,\n",
       "  'extremely': False,\n",
       "  'transfer': False,\n",
       "  'despite': False,\n",
       "  'cold': False,\n",
       "  'dividend': False,\n",
       "  'consider': False,\n",
       "  'gold': False,\n",
       "  'lady': False,\n",
       "  'crazy': False,\n",
       "  'tire': False,\n",
       "  'mco': False,\n",
       "  'kudos': False,\n",
       "  'afternoon': False,\n",
       "  'baby': False,\n",
       "  'tix': False,\n",
       "  'overnight': False,\n",
       "  'cut': False,\n",
       "  'course': False,\n",
       "  'spend': False,\n",
       "  'mobile': False,\n",
       "  'ice': False,\n",
       "  'sort': False,\n",
       "  'enjoy': False,\n",
       "  'control': False,\n",
       "  'shit': False,\n",
       "  'passbook': False,\n",
       "  'trouble': False,\n",
       "  'friday': False,\n",
       "  'dont': False,\n",
       "  'top': False,\n",
       "  'mind': False,\n",
       "  'haha': False,\n",
       "  'low': False,\n",
       "  'mia': False,\n",
       "  'b/c': False,\n",
       "  'airplane': False,\n",
       "  'code': False,\n",
       "  'window': False,\n",
       "  'social': False,\n",
       "  'plz': False,\n",
       "  'husband': False,\n",
       "  'mine': False,\n",
       "  'page': False,\n",
       "  'yr': False,\n",
       "  'birthday': False,\n",
       "  'hire': False,\n",
       "  'fl': False,\n",
       "  'feb': False,\n",
       "  'hello': False,\n",
       "  'middle': False,\n",
       "  'country': False,\n",
       "  'flightlation': False,\n",
       "  'bit': False,\n",
       "  'item': False,\n",
       "  'win': False,\n",
       "  'offering': False,\n",
       "  'track': False,\n",
       "  'attitude': False,\n",
       "  'listen': False,\n",
       "  'ppl': False,\n",
       "  'mistake': False,\n",
       "  'nope': False,\n",
       "  'select': False,\n",
       "  'area': False,\n",
       "  'advisory': False,\n",
       "  'literally': False,\n",
       "  'explanation': False,\n",
       "  'totally': False,\n",
       "  'unable': False,\n",
       "  'catch': False,\n",
       "  'volume': False,\n",
       "  'flights': False,\n",
       "  'learn': False,\n",
       "  'push': False,\n",
       "  'failure': False,\n",
       "  'meal': False,\n",
       "  'held': False,\n",
       "  '3rd': False,\n",
       "  'anyway': False,\n",
       "  'concern': False,\n",
       "  'cover': False,\n",
       "  'tag': False,\n",
       "  'overhead': False,\n",
       "  'bna': False,\n",
       "  'true': False,\n",
       "  'blame': False,\n",
       "  'americanairlines': False,\n",
       "  'worth': False,\n",
       "  'center': False,\n",
       "  'usairwaysfail': False,\n",
       "  'itinerary': False,\n",
       "  'ride': False,\n",
       "  'march': False,\n",
       "  'wonderful': False,\n",
       "  'son': False,\n",
       "  'feedback': False,\n",
       "  'spoke': False,\n",
       "  'hand': False,\n",
       "  'compensate': False,\n",
       "  'round': False,\n",
       "  'web': False,\n",
       "  'choose': False,\n",
       "  'saw': False,\n",
       "  'anywhere': False,\n",
       "  'however': False,\n",
       "  'mention': False,\n",
       "  'going': False,\n",
       "  'four': False,\n",
       "  'takeoff': False,\n",
       "  'behind': False,\n",
       "  'story': False,\n",
       "  'excellent': False,\n",
       "  'accommodate': False,\n",
       "  'enter': False,\n",
       "  'earn': False,\n",
       "  'non': False,\n",
       "  'receipt': False,\n",
       "  'apply': False,\n",
       "  'virgin': False,\n",
       "  'touch': False,\n",
       "  'dollar': False,\n",
       "  'neveragain': False,\n",
       "  'unhelpful': False,\n",
       "  'expire': False,\n",
       "  'warm': False,\n",
       "  'folk': False,\n",
       "  'honor': False,\n",
       "  'friendly': False,\n",
       "  'party': False,\n",
       "  'bumped': False,\n",
       "  'btw': False,\n",
       "  'flightd': False,\n",
       "  'assist': False,\n",
       "  'office': False,\n",
       "  'slow': False,\n",
       "  'locate': False,\n",
       "  'svc': False,\n",
       "  'regard': False,\n",
       "  'short': False,\n",
       "  'conf': False,\n",
       "  'inflight': False,\n",
       "  'ny': False,\n",
       "  'waiting': False,\n",
       "  'rate': False,\n",
       "  '1k': False,\n",
       "  'announce': False,\n",
       "  'trust': False,\n",
       "  'attempt': False,\n",
       "  'missed': False,\n",
       "  'excuse': False,\n",
       "  'absolutely': False,\n",
       "  'broke': False,\n",
       "  'clear': False,\n",
       "  'count': False,\n",
       "  'funeral': False,\n",
       "  'anymore': False,\n",
       "  'check-in': False,\n",
       "  'none': False,\n",
       "  'excite': False,\n",
       "  'total': False,\n",
       "  'saturday': False,\n",
       "  'bird': False,\n",
       "  'w/o': False,\n",
       "  'pull': False,\n",
       "  'special': False,\n",
       "  'onto': False,\n",
       "  'solution': False,\n",
       "  'airway': False,\n",
       "  'mexico': False,\n",
       "  'fan': False,\n",
       "  'domestic': False,\n",
       "  'except': False,\n",
       "  'phoenix': False,\n",
       "  'kept': False,\n",
       "  'impressed': False,\n",
       "  'frequent': False,\n",
       "  'philadelphia': False,\n",
       "  'major': False,\n",
       "  'bother': False,\n",
       "  'single': False,\n",
       "  'carrier': False,\n",
       "  'south': False,\n",
       "  'street': False,\n",
       "  '75': False,\n",
       "  'merger': False,\n",
       "  'sky': False,\n",
       "  'woman': False,\n",
       "  'break': False,\n",
       "  'program': False,\n",
       "  'walk': False,\n",
       "  'clean': False,\n",
       "  'reward': False,\n",
       "  'flying': False,\n",
       "  'especially': False,\n",
       "  'equipment': False,\n",
       "  '90': False,\n",
       "  'safe': False,\n",
       "  'train': False,\n",
       "  'entertainment': False,\n",
       "  'water': False,\n",
       "  'remember': False,\n",
       "  'seattle': False,\n",
       "  'important': False,\n",
       "  'become': False,\n",
       "  'difference': False,\n",
       "  'video': False,\n",
       "  'callback': False,\n",
       "  'text': False,\n",
       "  'america': False,\n",
       "  'funny': False,\n",
       "  '1hr': False,\n",
       "  'security': False,\n",
       "  'representative': False,\n",
       "  'onboard': False,\n",
       "  'sound': False,\n",
       "  'order': False,\n",
       "  'deny': False,\n",
       "  'n': False,\n",
       "  'tmrw': False,\n",
       "  'unhappy': False,\n",
       "  'cc': False,\n",
       "  'disgust': False,\n",
       "  'damage': False,\n",
       "  '35': False,\n",
       "  'event': False,\n",
       "  'bus': False,\n",
       "  'pathetic': False,\n",
       "  'sick': False,\n",
       "  'relation': False,\n",
       "  'rule': False,\n",
       "  'kill': False,\n",
       "  'rental': False,\n",
       "  'stuff': False,\n",
       "  'absolute': False,\n",
       "  'fyi': False,\n",
       "  'begin': False,\n",
       "  'disappoint': False,\n",
       "  'print': False,\n",
       "  'welcome': False,\n",
       "  'premier': False,\n",
       "  'god': False,\n",
       "  'forgot': False,\n",
       "  'suggestion': False,\n",
       "  'reimburse': False,\n",
       "  'tried': False,\n",
       "  'inside': False,\n",
       "  'badservice': False,\n",
       "  'nonstop': False,\n",
       "  'power': False,\n",
       "  'manage': False,\n",
       "  'raise': False,\n",
       "  'redeem': False,\n",
       "  'play': False,\n",
       "  'fair': False,\n",
       "  'shot': False,\n",
       "  'likely': False,\n",
       "  'iphone': False,\n",
       "  'gotten': False,\n",
       "  '60': False,\n",
       "  'suitcase': False,\n",
       "  'c': False,\n",
       "  'smh': False,\n",
       "  'nightmare': False,\n",
       "  'others': False,\n",
       "  'picked': False,\n",
       "  'advise': False,\n",
       "  'serious': False,\n",
       "  'acceptable': False,\n",
       "  'freeze': False,\n",
       "  'trueblue': False,\n",
       "  'base': False,\n",
       "  'fantastic': False,\n",
       "  'avgeek': False,\n",
       "  'ta': False,\n",
       "  'appease': False,\n",
       "  'usually': False,\n",
       "  'match': False,\n",
       "  'lovely': False,\n",
       "  '2.5': False,\n",
       "  'serve': False,\n",
       "  'per': False,\n",
       "  'imagine': False,\n",
       "  'resolution': False,\n",
       "  'view': False,\n",
       "  'auto': False,\n",
       "  'current': False,\n",
       "  'screen': False,\n",
       "  'diego': False,\n",
       "  'additional': False,\n",
       "  'unbelievable': False,\n",
       "  'columbus': False,\n",
       "  'reserve': False,\n",
       "  'catering': False,\n",
       "  'angry': False,\n",
       "  'require': False,\n",
       "  'exit': False,\n",
       "  'suggest': False,\n",
       "  'complain': False,\n",
       "  'damn': False,\n",
       "  'prefer': False,\n",
       "  'flightling': False,\n",
       "  'die': False,\n",
       "  'photo': False,\n",
       "  'visit': False,\n",
       "  'main': False,\n",
       "  'final': False,\n",
       "  'nc': False,\n",
       "  'fit': False,\n",
       "  'captain': False,\n",
       "  '2+': False,\n",
       "  '2hrs': False,\n",
       "  '😊': False,\n",
       "  'willing': False,\n",
       "  'discount': False,\n",
       "  'appear': False,\n",
       "  'internet': False,\n",
       "  'act': False,\n",
       "  'ignore': False,\n",
       "  'foot': False,\n",
       "  'bin': False,\n",
       "  'emergency': False,\n",
       "  'avoid': False,\n",
       "  'result': False,\n",
       "  'usa': False,\n",
       "  'upgraded': False,\n",
       "  ...},\n",
       " 'negative')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd4b224-7e08-4320-bfa2-be981f731552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7727272727272727"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, tweets_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "427d341f-6e38-4b88-be54-34e857c4e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                passbook = True           positi : negati =     40.4 : 1.0\n",
      "                 amazing = True           positi : negati =     32.6 : 1.0\n",
      "                favorite = True           positi : negati =     30.0 : 1.0\n",
      "             outstanding = True           positi : negati =     27.4 : 1.0\n",
      "                discount = True           neutra : negati =     26.9 : 1.0\n",
      "                   kudos = True           positi : negati =     25.2 : 1.0\n",
      "                    rude = True           negati : neutra =     24.4 : 1.0\n",
      "                 helpful = True           positi : neutra =     23.1 : 1.0\n",
      "                 awesome = True           positi : negati =     22.8 : 1.0\n",
      "               beautiful = True           positi : negati =     22.2 : 1.0\n",
      "                    rock = True           positi : negati =     22.2 : 1.0\n",
      "                   smile = True           positi : negati =     22.2 : 1.0\n",
      "               wonderful = True           positi : negati =     21.8 : 1.0\n",
      "                      hr = True           negati : positi =     21.7 : 1.0\n",
      "                   thank = True           positi : negati =     21.0 : 1.0\n",
      "                  online = True           negati : positi =     21.0 : 1.0\n",
      "                  street = True           neutra : negati =     20.9 : 1.0\n",
      "                      wo = True           negati : positi =     20.5 : 1.0\n",
      "                   flyfi = True           positi : negati =     19.6 : 1.0\n",
      "                terrible = True           negati : neutra =     18.4 : 1.0\n",
      "                     ceo = True           neutra : positi =     16.5 : 1.0\n",
      "                   happy = True           positi : neutra =     16.5 : 1.0\n",
      "                   sweet = True           positi : negati =     16.4 : 1.0\n",
      "                       👍 = True           positi : negati =     16.4 : 1.0\n",
      "                   great = True           positi : neutra =     16.2 : 1.0\n",
      "                  battle = True           neutra : negati =     16.1 : 1.0\n",
      "               excellent = True           positi : negati =     16.1 : 1.0\n",
      "                    hold = True           negati : positi =     15.6 : 1.0\n",
      "                  rebook = True           negati : positi =     15.6 : 1.0\n",
      "                   among = True           neutra : negati =     14.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdecdc5-a690-4a8b-9132-045e8ace1b99",
   "metadata": {},
   "source": [
    "### **Sklearn Classifiers within NLTK**\n",
    "\n",
    "There is a Sklearn classifier that gives uses of NLTK a way to call the underlying scikit-learn classifier through their code in Phyton.\n",
    "\n",
    "To construct a scikit-learn estimator object, then use that to construct a SklearnClassifier. E.g., to wrap a linear SVM with default settings:\n",
    "\n",
    "$$from \\;sklearn.svm \\;import LinearSVC$$\n",
    "\n",
    "$$from\\; nltk.classify.scikitlearn\\; import\\; SklearnClassifier$$\n",
    "\n",
    "$$classifier = SklearnClassifier(LinearSVC())$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80d441-faee-4afb-8a6a-3ca8205b0271",
   "metadata": {},
   "source": [
    "### Classification using SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13264f4e-0599-4e6c-910e-e056162ff2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Sklearn Classifier within Nltk\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "583bac03-6444-4125-bb06-9b97771a77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "classifier_sklearn = SklearnClassifier(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56052fe6-5d8f-4e8e-814e-d0e0374952fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC())>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sklearn.train(tweets_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9962221-5d39-47bc-bf7c-f6c5956fef57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7791666666666667"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn, tweets_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf059a-2e3a-43bc-88cf-3c4d0c8c003d",
   "metadata": {},
   "source": [
    "### Classification using Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc2943da-d1ad-4fc2-935d-dc8ac8c2cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "classifier_sklearn2 = SklearnClassifier(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe30e910-74e2-4946-82f1-66f357bd7afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(RandomForestClassifier())>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sklearn2.train(tweets_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41030590-89bb-42f0-8bc6-b94644d1e6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.765530303030303"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn2, tweets_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8ac6a-1363-45dc-b6ea-7c97cceaf885",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "Count Vectorizer is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.\n",
    "\n",
    "It is used to convert a collection of text documents to a vector of term/token counts. It also enables the ​pre-processing of text data prior to generating the vector representation.\n",
    "\n",
    "CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e34af6d-ebe0-4b4a-99f1-31389bf55281",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_train = [sentiment for tweet, sentiment in tweets_data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc640c68-4c52-4df1-a650-98c591e9892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_test= [sentiment for tweet, sentiment in tweets_data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52fbf607-6acb-400d-abca-8fae72d0459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_train = [' '.join(tweet) for tweet, sentiment in tweets_data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43057c13-4855-4c77-9e39-83d7b2c99d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_test = [' '.join(tweet) for tweet, sentiment in tweets_data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cae7d811-0dae-4f00-9d61-524fa0ef90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(max_features = 8000)\n",
    "tweets_data_train_vec = count_vec.fit_transform(tweets_data_train)\n",
    "tweets_data_test_vec = count_vec.transform(tweets_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "182552d4-8575-46eb-916e-30db861c4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# clf = SVC()\n",
    "# grid = {'C': [1e2, 1e3, 5e3, 1e4, 5e4, 1e5], 'gamma': [1e-3, 5e-4, 1e-4, 5e-3]}\n",
    "# abc = GridSearchCV(clf, grid)\n",
    "# abc.fit(tweets_data_train, sentiments)\n",
    "# abc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4014febe-671c-4cdc-8cfb-e28f22788007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178030303030303"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(tweets_data_train_vec, sentiments_train)\n",
    "svc.score(tweets_data_test_vec, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe8818-4957-47d8-afdb-f92e8785f097",
   "metadata": {},
   "source": [
    "### **N-Grams**\n",
    "\n",
    "An N-gram is an N-token sequence of words: a 2-gram (called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (trigram) is a three-word sequence of wor¯ds like “not at all”, or “turn off light”.\n",
    "\n",
    "Set the parameter ngram_range=(a,b) where a is the minimum and b is the maximum size of ngrams you want to include in your features. The default ngram_range is (1,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a319f-d99c-424b-8be2-6ed86f75df0b",
   "metadata": {},
   "source": [
    "Instead of using a single word as feature, we can use a pair of words or three words as one features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39504d96-241c-4f7c-aa53-d49446a4f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features = 6000, ngram_range=(2,3))\n",
    "tweets_data_train_vec = count_vec.fit_transform(tweets_data_train)\n",
    "tweets_data_test_vec = count_vec.transform(tweets_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "053db693-1b29-4540-bebc-a9062021d007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178030303030303"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(tweets_data_train_vec, sentiments_train)\n",
    "svc.score(tweets_data_test_vec, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891269ec-e9f8-43e8-88ed-5ed396452a1a",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5a99e-9592-441c-8a4e-75f58f5c62c3",
   "metadata": {},
   "source": [
    "Classification using NLTK Naive Bayes: 0.77\n",
    "\n",
    "Classification using Sklearn SVC: 0.78\n",
    "\n",
    "Classification using Sklearn Random Forest: 0.76\n",
    "\n",
    "Classification using Count Vectorizer and SVC: 0.62\n",
    "\n",
    "Classification using Count Vectorizer with N-Grams and SVC: 0.62"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
